
"""
 Trains a defined model with a defined ratio of malware samples in each dataset and saves the model afterwards.
"""

#TensorFlow and tf.keras
import tensorflow as tf
from tensorflow import keras

# Helper libraries
import numpy as np
from sklearn import metrics
import matplotlib.pyplot as plt
from tensorflow.keras.utils import plot_model

import os
from random import randint, shuffle

FILE_MALWARES = '../data/malwares_sha256.txt' # path to file with the SHAs of all malware samples from the dataset
DIR_APP_FEATURES = '../data/apps_feature_indexes_reduced/' # directory containing all definition files

# list with all malware samples
MALWARES = []
# list with all cleanware samples
CLEANWARES = []

print("creating list of MALWARES...")
with open(FILE_MALWARES, "r") as file:
    for line in file:
        if line:
            MALWARES.append(line.strip())
print("finished!")

print("creating list of CLEANWARES...")
for filename in os.listdir(DIR_APP_FEATURES):
    if filename not in MALWARES:
        CLEANWARES.append(filename)
print("finished!")


#=========== CLASSIFIER ============== CLASSIFIER ============== CLASSIFIER ===========
#===================================== PARAMETERS =====================================
M = 4109
BATCH_SIZE = 1000
DROPOUT = 0.5
EPOCHS = 10
#===================================== DEFINITION =====================================
def sequential_model(hidden_units):
    """
    Creates the deep neural network with the given structure

    :param hidden_units: list containing the amount of neurons per hidden layer
    :return: DNN
    """
    model = keras.Sequential()

    # add hidden layers
    model.add(keras.layers.Dense(hidden_units[0], activation=tf.nn.relu, input_dim=M)) # input layer, so we need to
                                                                                       # define the input dimension
                                                                                       # (input_dim)
    model.add(keras.layers.Dropout(rate=DROPOUT))
    for units in hidden_units[1:]:
        model.add(keras.layers.Dense(units, activation=tf.nn.relu))
        model.add(keras.layers.Dropout(rate=DROPOUT))

    # add output layer
    model.add(keras.layers.Dense(2, activation=tf.nn.softmax, name='output-layer'))

    # compile the model
    model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy,
            optimizer=tf.keras.optimizers.Adam(),
            metrics=['accuracy'])
    model.summary()

    return model
#======================================================================================
#======================================================================================

def generate_app_set(ratio):
    app_set= []
    # add MALWARES to training data set
    while len(app_set) < (BATCH_SIZE * ratio):
        index = randint(0, len(MALWARES)-1)
        app = MALWARES[index]
        if app not in app_set:
            app_set.append(app)
    # add CLEANWARES to training data set
    while len(app_set) < BATCH_SIZE:
        index = randint(0, len(CLEANWARES)-1)
        app = CLEANWARES[index]
        if app not in app_set:
            app_set.append(app)
    return app_set

def generate_model_input(applist):
    data = np.zeros((len(applist), M), dtype=float)
    labels = np.zeros((len(applist),), dtype=int)

    shuffle(applist)
    for idx, app_sha in enumerate(applist):
        app_sha = app_sha.strip()
        with open(DIR_APP_FEATURES + app_sha, 'r') as app:
            for index in app:
                data[idx][int(index)] = 1.0

        if app_sha in MALWARES:
            labels[idx] = 1
        else:
            labels[idx] = 0

    return data, labels

# path to the directory in that the model should be saved
MODEL_DIR = './models/DNN-feature-reduced_200-200_'
# list with the amount of neurons per hidden layer (currently: 2 hidden layers with 200 neurons each)
MODEL_CONFIG = [200,200]
# malware ratio with that the model should be trained
RATIO = 0.3
# size of the mini-batches for training
STEP_SIZE = 100

MODEL_DIR = MODEL_DIR + str(RATIO)
model = sequential_model(hidden_units=MODEL_CONFIG)

# create tensorboard callback
tbCallBack = keras.callbacks.TensorBoard(log_dir=MODEL_DIR, histogram_freq=0, write_graph=True, write_images=True)
# Create checkpoint callback
cpCallBack = tf.keras.callbacks.ModelCheckpoint(MODEL_DIR, save_weights_only=True, verbose=1, period=EPOCHS)

for i in range(5):
    print('-------------------- TRAINING Ratio ({}) --------------------'.format(ratio))
    train_apps = generate_app_set(ratio)
    train_data, train_labels = generate_model_input(train_apps)

    for x in range(0, len(train_apps), STEP_SIZE):
        model.fit(train_data[x:x+STEP_SIZE], train_labels[x:x+STEP_SIZE], epochs=EPOCHS, verbose=2,
                callbacks=[tbCallBack,cpCallBack])
    # ======================================================================================

    print('-------------------- TESTING Ratio ({}) --------------------'.format(ratio))
    test_apps = generate_app_set(ratio)
    test_data, test_labels = generate_model_input(test_apps)
    results = model.evaluate(test_data, test_labels, verbose=2)
    print("Error:\t{0:f}\nAccuracy:\t{1:f}%".format(results[0], results[1]*100))
    # ======================================================================================

    print('-------------------- VALIDATION Ratio ({}) --------------------'.format(ratio))
    val_apps = generate_app_set(ratio)
    val_data, val_labels = generate_model_input(val_apps)
    predictions = model.predict(val_data)
    confusion = metrics.confusion_matrix(val_labels, np.argmax(predictions, axis=1))
    print(confusion)
    TP = confusion[1, 1]
    TN = confusion[0, 0]
    FP = confusion[0, 1]
    FN = confusion[1, 0]
    FNR = FN / float(FN+TP)
    FPR = FP / float(FP+TN)
    ACC = (TP + TN) / float(TP+TN+FP+FN)
    print('FP: {}\tFN: {}\t\tTP: {}\tTN: {}'.format(FP,FN,TP,TN))
    print('Accuracy: {}\tFPR: {}\tFNR: {}\n'.format(ACC,FPR,FNR))

# save the model
model.save(MODEL_DIR + '.h5')
print('\n\n\n')
