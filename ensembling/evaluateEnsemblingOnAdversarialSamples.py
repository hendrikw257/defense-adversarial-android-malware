
#TensorFlow and tf.keras
import tensorflow as tf
from tensorflow import keras

# Helper libraries
import numpy as np
from sklearn import metrics
import matplotlib.pyplot as plt
from tensorflow.keras.utils import plot_model
from sklearn.neighbors import KNeighborsClassifier

import os
from random import randint, shuffle
from sklearn.externals import joblib

FILE_MALWARES = '../data/malwares_sha256.txt' # path to file with the SHAs of all malware samples from the dataset
DIR_APP_FEATURES = '../data/apps_feature_indexes/' # directory containing all definition files

# list with all malware samples
MALWARES = []
# list with all cleanware samples
CLEANWARES = []

print("creating list of MALWARES...")
with open(FILE_MALWARES, "r") as file:
    for line in file:
        if line:
            MALWARES.append(line.strip())
print("finished!")

print("creating list of CLEANWARES...")
for filename in os.listdir(DIR_APP_FEATURES):
    if filename not in MALWARES:
        CLEANWARES.append(filename)
print("finished!")


#=========== CLASSIFIER ============== CLASSIFIER ============== CLASSIFIER ===========
#===================================== PARAMETERS =====================================
M = 545333
BATCH_SIZE = 1000
DROPOUT = 0.5
EPOCHS = 10
# ======================================================================================
# ======================================================================================
def generate_app_set(ratio):
    app_set = []
    # add MALWARES to training data set
    while len(app_set) < (BATCH_SIZE * ratio):
        index = randint(0, len(MALWARES) - 1)
        app = MALWARES[index]
        if app not in app_set:
            app_set.append(app)
    # add CLEANWARES to training data set
    while len(app_set) < BATCH_SIZE:
        index = randint(0, len(CLEANWARES) - 1)
        app = CLEANWARES[index]
        if app not in app_set:
            app_set.append(app)
    return app_set


def generate_model_input(applist):
    data = np.zeros((len(applist), M), dtype=float)
    labels = np.zeros((len(applist),), dtype=int)

    shuffle(applist)
    for idx, app_sha in enumerate(applist):
        app_sha = app_sha.strip()
        with open(DIR_APP_FEATURES + app_sha, 'r') as app:
            for index in app:
                data[idx][int(index)] = 1.0

        if app_sha in MALWARES:
            labels[idx] = 1
        else:
            labels[idx] = 0

    return data, labels

def sum_probs(dnn_probs, knn_probs):
    dnn_weight = 0.6
    knn_weight = 1 - dnn_weight
    summed = []
    for i in range(len(dnn_probs)):
        summed.append([])
        for j in range(len(dnn_probs[i])):
            summed[i].append((dnn_weight * dnn_probs[i][j]) + (knn_weight * knn_probs[i][j]))
    return summed

# --------------------------------------------------------------------------------------
'''
 functions to compute jacobian matrix of DNN 
 (source: https://github.com/hrzn/jacobianmatrix/blob/master/Jacobian-matrix-examples.ipynb)
'''
# --------------------------------------------------------------------------------------
def affine_forward(x, w, b):
    """
    Forward pass of an affine layer
    :param x: input of dimension (D, )
    :param w: weights matrix of dimension (D, M)
    :param b: biais vector of dimension (M, )
    :return output of dimension (M, ), and cache needed for backprop
    """
    out = np.dot(x, w) + b
    cache = (x, w)
    return out, cache


def affine_backward(dout, cache):
    """
    Backward pass for an affine layer.
    :param dout: Upstream Jacobian, of shape (O, M)
    :param cache: Tuple of:
      - x: Input data, of shape (D, )
      - w: Weights, of shape (D, M)
    :return the jacobian matrix containing derivatives of the O neural network outputs with respect to
            this layer's inputs, evaluated at x, of shape (O, D)
    """
    x, w = cache
    dx = np.dot(dout, w.T)
    return dx


def relu_forward(x):
    """ Forward ReLU
    """
    out = np.maximum(np.zeros(x.shape), x)
    cache = x
    return out, cache


def relu_backward(dout, cache):
    """
    Backward pass of ReLU
    :param dout: Upstream Jacobian
    :param cache: the cached input for this layer
    :return: the jacobian matrix containing derivatives of the O neural network outputs with respect to
             this layer's inputs, evaluated at x.
    """
    x = cache
    dx = dout * np.where(x > 0, np.ones(x.shape), np.zeros(x.shape))
    return dx


def softmax_forward(x):
    """ Forward softmax
    """
    exps = np.exp(x - np.max(x))
    s = exps / exps.sum()
    return s, s


def softmax_backward(dout, cache):
    """
    Backward pass for softmax
    :param dout: Upstream Jacobian
    :param cache: contains the cache (in this case the output) for this layer
    """
    s = cache
    ds = np.diag(s) - np.outer(s, s.T)
    dx = np.dot(dout, ds)
    return dx
# --------------------------------------------------------------------------------------

def get_activations(model, layer_id, X):
    """
    Computes outputs of intermediate layers
    :param model: DNN
    :param layer_id: ID of the layer, you want the output from
    :param X: input feature vector
    :return: output of layer with ID 'layer_id'
    """
    intermediate_layer_model = keras.models.Model(inputs=model.input,
                                                  outputs=model.layers[layer_id].output)
    intermediate_output = intermediate_layer_model.predict(X)
    return intermediate_output


def forward_backward(model, x):
    """
    computes the forward derivative for the given input
    :param model: DNN
    :param x: input feature vector
    :return: prediction result and forward derivative
    """
    layer_to_cache = dict()  # for each layer, we store the cache needed for backward pass
    forward_values = []

    for i in xrange(0, len(model.layers), 2):
        values = {}
        w, b = model.layers[i].get_weights()
        values['w'] = w
        values['b'] = b
        forward_values.append(values)

    # Forward pass
    a1, cache_a1 = affine_forward(x, forward_values[0]['w'], forward_values[0]['b'])
    _, cache_r1 = relu_forward(a1)
    r1 = get_activations(model, 0, x)
    forward_values[0]['a'] = a1
    forward_values[0]['cache_a'] = cache_a1
    forward_values[0]['r'] = r1
    forward_values[0]['cache_r'] = cache_r1

    for i, layer_index in zip(range(1, len(forward_values) - 1), range(2, len(model.layers), 2)):
        a, cache_a = affine_forward(forward_values[i - 1]['r'], forward_values[i]['w'], forward_values[i]['b'])
        _, cache_r = relu_forward(a)
        r = get_activations(model, layer_index, x)
        forward_values[i]['a'] = a
        forward_values[i]['cache_a'] = cache_a
        forward_values[i]['r'] = r
        forward_values[i]['cache_r'] = cache_r

    a, cache_a = affine_forward(forward_values[len(forward_values) - 2]['r'],
                                forward_values[len(forward_values) - 1]['w'],
                                forward_values[len(forward_values) - 1]['b'])
    forward_values[len(forward_values) - 1]['a'] = a
    forward_values[len(forward_values) - 1]['cache_a'] = cache_a
    out, cache_out = softmax_forward(a)

    # backward pass
    dout = np.diag(np.ones(out.size, ))  # the derivatives of each output w.r.t. each output.
    dout = softmax_backward(dout, cache_out)
    dout = affine_backward(dout, forward_values[len(forward_values) - 1]['cache_a'])

    for i in range(len(forward_values) - 2, 0, -1):
        dout = relu_backward(dout, forward_values[i]['cache_r'])
        dout = affine_backward(dout, forward_values[i]['cache_a'])

    dout = relu_backward(dout, forward_values[0]['cache_r'])
    dx = affine_backward(dout, forward_values[0]['cache_a'])

    return out, dx
# ============================================================================================================
# ============================================================================================================
def craft_adversarial_samples(x, y, F, k):
    """
    Crafting algorithm as defined in the thesis
    :param x: input feature vector
    :param y: target class
    :param F: DNN
    :param k: index of the hidden layer
    :return: adversarial sample based on feature vector x
    """
    x_adv = x
    # print('|1| - {}\t{}'.format(F.predict(x_adv),np.argmax(F.predict(x_adv), 1)))

    upgamma = [1] * len(x)
    delta_x = [0]
    changes = 0

    if np.argmax(F.predict(x_adv), 1) == 0:
        return x_adv, -1

    while np.argmax(F.predict(x_adv), 1) != y and np.linalg.norm(delta_x, ord=1) < k and changes < 20:
        # compute forward derivative
        prob, forward_derivative = forward_backward(F, x_adv)

        tmp = np.multiply(forward_derivative[0], upgamma)
        for i, feature in enumerate(x_adv[0]):
            if feature == 1:
                tmp[i] = 0
        i_max = np.argmax(tmp)
        # i_max = np.argmax(np.multiply(forward_derivative[0],upgamma))
        if i_max <= 0:
            raise ValueError('FAILURE: i_max <= 0 - it is only possible to add features to an application!')

        x_adv[0][i_max] = 1
        delta_x = np.subtract(x_adv, x)
        # print('|2| - {}\t{}'.format(F.predict(x_adv), np.argmax(F.predict(x_adv), 1)))
        changes += 1
    # print(changes)
    # print('\n')
    return x_adv, changes
#============================================================================================================

knn_exists = True
# path to the directory in that the DNN model was saved
DNN_MODEL_DIR = '../basis/models/DNN_200-200_{}.h5'
# path to the directory in that the KNN model was saved
KNN_MODEL_DIR = './models/KNN_{}.sav'
# malware ratio with that the model should be evaluated
RATIO = 0.3

if not knn_exists:
    knnmodel = KNeighborsClassifier(n_neighbors = 3)
    for i in range(5):
        train_apps = generate_app_set(RATIO)
        train_data, train_labels = generate_model_input(train_apps)
        for epoch in range(1):
            # train KNN model
            knnmodel.fit(train_data, train_labels)
    del train_apps
    del train_data
    del train_labels
else:
    KNN_MODEL_DIR = KNN_MODEL_DIR.format(RATIO)
    knnmodel = joblib.load(KNN_MODEL_DIR)


DNN_MODEL_DIR = DNN_MODEL_DIR.format(RATIO)
dnn_model = tf.keras.models.load_model(DNN_MODEL_DIR)

averageAcc = 0
averageAcc1 = 0
averageFPR = 0
averageFNR = 0
averageChanges = 0
runs = 2
for index in range(runs):
    val_apps = generate_app_set(RATIO)
    val_data, val_labels = generate_model_input(val_apps)

    average_changes = 0
    amount_malwares = 0
    for i in range(len(val_data)):
        if val_labels[index] == 1:
            x = val_data[i:i+1]

            try:
                adv_x, changes = craft_adversarial_samples(x, 0, dnn_model, 1)
                val_data[i] = adv_x
                if changes >= 0:
                    average_changes += changes
                    amount_malwares += 1
            except ValueError:
                pass

    if amount_malwares > 0:
        averageChanges += (average_changes/float(amount_malwares))

    dnnpredictions = dnn_model.predict(val_data)
    knnpredictions = knnmodel.predict_proba(val_data)
    predictions = sum_probs(dnnpredictions, knnpredictions)

    confusion = metrics.confusion_matrix(val_labels, np.argmax(predictions, axis=1))
    TP = confusion[1, 1]
    TN = confusion[0, 0]
    FP = confusion[0, 1]
    FN = confusion[1, 0]
    FNR = FN / float(FN+TP)
    FPR = FP / float(FP+TN)
    ACC = ((TP + TN) / float(TP+TN+FP+FN))*100
    averageAcc += ACC
    averageFNR += FNR
    averageFPR += FPR
    print('FP: {}\tFN: {}\t\tTP: {}\tTN: {}'.format(FP,FN,TP,TN))
    print('Accuracy: {}\tFPR: {}\tFNR: {}\n'.format(ACC,FPR,FNR))
print('Model {} with ratio {}'.format(DNN_MODEL_DIR,RATIO))
averageAcc = averageAcc/runs
averageFPR = averageFPR/runs
averageFNR = averageFNR/runs
print('Accuracy: {}\tMisclassification Rate: {}\tFPR: {}\tFNR: {}\n'.format(averageAcc,100-averageAcc,averageFPR,averageFNR))
print('Distortion: {}\n'.format(averageChanges/runs))
print('\n\n\n')

